{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning GAN - Structural Health Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-745b08f76377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#main file for training GAN\n",
    "#Author Bill Zhai, Dec 2019\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clas+ 0has 9600\n",
      "clas+ 1has 600\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-361f55cccde0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mlabeled_indices_by_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_is\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0munlabeled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpyData/0.25_labeled_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mnumTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_label_by_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "task_name = \"BINARY_CR_16_1_split_128_128\"\n",
    "npSeed = 188\n",
    "np.random.seed(npSeed)\n",
    "x_height, x_width = [128, 128]\n",
    "num_channels = 3\n",
    "num_classes = 2 #Undamaged, cracked/spalling\n",
    "latent_size = 100\n",
    "labeled_rate = 0.25 #1.0, 0.5, 0.1 this limits the knowledge base of the learning\n",
    "unlabeled_supp_rate = 0 # percentage of unlabeled data to be supplemented to the learning\n",
    "c_ul = 1\n",
    "# task_path = 'numpyData/BINARY_CR_16_1_split_128_128'\n",
    "# When you wake up, UNCOMMENT THIS line below****\n",
    "task_path = 'numpyData/'+task_name\n",
    "# All data (labeled and unlabeled) by class\n",
    "train_data_by_class, test_data_by_class, train_label_by_class, test_label_by_class = [], [], [], []\n",
    "train_mask_by_class = []\n",
    "#unlabeled_indices_by_class = []\n",
    "labeled_indices_by_class = [] # Marked for labeled data selection to batch\n",
    "\n",
    "unlabeled_data = np.zeros((0, x_width, x_height, num_channels)) # In order to randomly select from\n",
    "# all unlabeled data without considering classes\n",
    "labeled_data_baseline = np.zeros((0, x_width, x_height, num_channels)) # In order to export 0.25 labeled \n",
    "# Load real data:\n",
    "for i in range(num_classes):\n",
    "    train_data_by_class.append(np.load(task_path+'/class_'+str(i)+'/trainX.npy'))\n",
    "    train_label_by_class.append(np.load(task_path+'/class_'+str(i)+'/trainy.npy'))\n",
    "                        \n",
    "    test_data_by_class.append(np.load(task_path+'/class_'+str(i)+'/testX.npy'))\n",
    "    test_label_by_class.append(np.load(task_path+'/class_'+str(i)+'/testy.npy'))\n",
    "    print(\"clas+ \"+str(i)+'has '+str(len(train_label_by_class[i])))\n",
    "\n",
    "    \n",
    "    # Select data as unlabeled:\n",
    "for i in range(num_classes):\n",
    "    numInClass = len(train_label_by_class[i])\n",
    "    #mask =  np.concatenate((np.ones(int(numInClass * labeled_rate), dtype='int'), np.zeros(numInClass - int(numInClass * labeled_rate), dtype='int')), axis=0)\n",
    "    num_unlabeled = int(numInClass-numInClass*labeled_rate)\n",
    "    train_data_class = train_data_by_class[i]\n",
    "    train_label_class = train_data_by_class[i]\n",
    "    indices = [i for i in range(numInClass)]\n",
    "    np.random.shuffle(indices)\n",
    "    unlabeled_is, labeled_is = np.split(indices,[num_unlabeled]) # Unlabeled and labeled indices\n",
    "    \n",
    "    # Concatenate image data array\n",
    "    unlabeled_data = np.concatenate((unlabeled_data, train_data_by_class[i][unlabeled_is]))\n",
    "    labeled_data_baseline = np.concatenate((labeled_data_baseline, train_data_by_class[i][labeled_is]))\n",
    "    \n",
    "    #unlabeled_indices_by_class.append(unlabeled_is)\n",
    "    labeled_indices_by_class.append(labeled_is)\n",
    "\n",
    "np.save(unlabeled_data, 'numpyData/0.25_labeled_data')\n",
    "    \n",
    "numTrain = sum([len(c) for c in train_label_by_class])\n",
    "numTest = sum([len(c) for c in test_label_by_class])\n",
    "log_path = './SSL_GAN_log_' + task_name + '.csv' # Don't worry about log_path, it is named after the task\n",
    "log_path_baseline = './baseline_log.csv'\n",
    "model_path ='./savedModels/GAN'\n",
    "baseline_path = './savedModels/baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4594, 4914, 6163, ..., 6263, 8001, 8250]),\n",
       " array([489, 152, 411, 406, 534,  63, 192, 566, 290,  23, 333, 167, 239,\n",
       "        506, 569, 475, 217, 491, 574, 473, 232, 530,  88, 183, 520,  99,\n",
       "        212, 132,   9, 158,  28, 131, 378,   8, 352, 318, 454, 221, 251,\n",
       "        595, 423, 257, 456, 193, 449, 400, 304,  79, 443, 509,  21, 273,\n",
       "        325, 197, 432,  97, 415, 485, 209, 447, 103, 327, 130, 260, 572,\n",
       "        162, 386,  49,  13, 399, 402, 151, 419, 294,  95, 396, 561,  65,\n",
       "        236, 271, 554, 502, 312, 436,  48, 341, 548, 227, 585, 544, 551,\n",
       "        102, 145, 274, 384, 155, 570, 435, 316, 442, 445, 267, 543, 490,\n",
       "         89, 300, 459, 556, 185, 106, 278, 109, 173,  19, 138, 366, 182,\n",
       "        537, 289, 559,  70, 247,  82, 287, 198,  68, 383, 481,  87, 218,\n",
       "        549, 263, 523, 526, 538, 101, 137, 482, 434, 511,  62, 465,  60,\n",
       "        597,  40, 361,  46, 464, 552, 345])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_indices_by_class\n",
    "#[array([4594, 4914, 6163, ..., 6263, 8001, 8250]),\n",
    "# array([489, 152, 411, 406, 534,  63, 192, 566, 290,  23, 333, 167, 239,\n",
    " #       506, 569, 475, 217, 491, 574, 473, 232, 530,  88, 183, 520,  99,\n",
    "        #212, 132,   9, 158,  28, 131, 378,   8, 352, 318, 454, 221, 251,\n",
    "        #595, 423, 257, 456, 193, 449, 400, 304,  79, 443, 509,  21, 273,\n",
    "        #325, 197, 432,  97, 415, 485, 209, 447, 103, 327, 130, 260, 572,\n",
    "        #162, 386,  49,  13, 399, 402, 151, 419, 294,  95, 396, 561,  65,\n",
    "        #236, 271, 554, 502, 312, 436,  48, 341, 548, 227, 585, 544, 551,\n",
    "        #102, 145, 274, 384, 155, 570, 435, 316, 442, 445, 267, 543, 490,\n",
    "        # 89, 300, 459, 556, 185, 106, 278, 109, 173,  19, 138, 366, 182,\n",
    "       # 537, 289, 559,  70, 247,  82, 287, 198,  68, 383, 481,  87, 218,\n",
    "       # 549, 263, 523, 526, 538, 101, 137, 482, 434, 511,  62, 465,  60,\n",
    "        #597,  40, 361,  46, 464, 552, 345])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # normalize data\n",
    "#     x /= 255.0\n",
    "    x = (x - 127.5) / 127.5\n",
    "    return x.reshape((-1, x_height, x_width, 3)) #x is 4 dimensional-- (num_images, height, width, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save masked/labeled image to numpy array\n",
    "def save_masked(toFolder):\n",
    "    for i in range(num_classes):\n",
    "        if not os.path.exists('numpyData/'+toFolder+'/class_'+str(i)):\n",
    "            os.makedirs('numpyData/'+toFolder+'/class_'+str(i))\n",
    "        np.save('numpyData/'+toFolder+'/class_'+str(i)+'/trainX', train_data_by_class[i][train_mask_by_class[i].astype(bool)])\n",
    "        np.save('numpyData/'+toFolder+'/class_'+str(i)+'/trainy', train_label_by_class[i][train_mask_by_class[i].astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_masked(\"labeled_for_cnn_seed_\"+str(npSeed)+'_rate_'+str(labeled_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shuffle data array, labels array, and labeledMask array, each image's properties should remain consistant (labeled/unlabeled)\n",
    "#during the whole experiment. Instead of assigning labeled mask per next batch, it should be globally defined\n",
    "#prior to running the experiment --BZ, August, 2019\n",
    "def shuffle_data(data, labels, labeledMask):#all arrays here are row vectors? labels and data are columnwise\n",
    "    #np.random.seed(123)#for debugging purpose\n",
    "    indices = np.arange(labels.shape[0]) #index sequence whose length = len(labels)\n",
    "    np.random.shuffle(indices) #In place\n",
    "    shuffled_indices = indices #Useless assignment for clarity- shuffle is in place\n",
    "    if labeledMask is None:#for cases used by get_test_batch() function--BZ, August, 2019\n",
    "        return data[shuffled_indices], labels[shuffled_indices]\n",
    "    else:\n",
    "        return data[shuffled_indices], labels[shuffled_indices], labeledMask[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new batch functions:--BZ, August, 2019\n",
    "#because labeled mask should stay with the same images after each shuffle (which happens at the start of each new epoch),\n",
    "#get_batch() and get_labeled_mask() should be merged into one function\n",
    "def get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize):\n",
    "    #first shuffle the indices:\n",
    "    XTrainRandom, yTrainRandom, labeledMaskRandom = shuffle_data(XTrain, yTrain, labeledMask);\n",
    "    #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "    counter = 0;\n",
    "    while True:\n",
    "        if counter >= len(yTrain):\n",
    "            break;\n",
    "        returnXTrain = XTrainRandom[counter:counter + batchSize];\n",
    "        returnYTrain = yTrainRandom[counter:counter + batchSize];\n",
    "        returnLabeledMask = labeledMaskRandom[counter:counter + batchSize];\n",
    "        counter = counter + batchSize;\n",
    "        yield returnXTrain, returnYTrain, returnLabeledMask\n",
    "def get_test_batch(XTest, yTest, batchSize): #essentially the same function as above, restated here for explicity\n",
    "    #first shuffle the indices:\n",
    "    XTestRandom, yTestRandom = shuffle_data(XTest, yTest, None);\n",
    "    #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "    counter = 0;\n",
    "    while True:\n",
    "        if counter >= len(yTest):\n",
    "            break;\n",
    "        returnXTest = XTestRandom[counter:counter + batchSize];\n",
    "        returnYTest = yTestRandom[counter:counter + batchSize];\n",
    "        counter = counter + batchSize;\n",
    "        yield returnXTest, returnYTest\n",
    "    \n",
    "def get_balance_train_batch(train_data_by_class, train_label_by_class, train_mask_by_class, batchSize):\n",
    "    numEachClass = int(np.floor(batchSize / num_classes))\n",
    "    returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "    returnLabel = np.zeros((0, num_classes))\n",
    "    returnLabeledMask = np.zeros((0))\n",
    "    for i in range(num_classes):\n",
    "        train_data = train_data_by_class[i]\n",
    "        train_label = train_label_by_class[i]\n",
    "        train_mask = train_mask_by_class[i]\n",
    "        indices = [v for v in range(len(train_label))]\n",
    "        np.random.shuffle(indices)\n",
    "        selectIndices = indices[0:numEachClass]\n",
    "        returnData = np.concatenate((returnData, train_data[selectIndices]))\n",
    "        returnLabel = np.concatenate((returnLabel, train_label[selectIndices]))\n",
    "        returnLabeledMask = np.concatenate((returnLabeledMask, train_mask[selectIndices]))\n",
    "    return returnData, returnLabel, returnLabeledMask\n",
    "\n",
    "def get_balance_train_batch_2(train_data_by_class, train_label_by_class, train_mask_by_class, batchSize):\n",
    "    numEachClass = int(batchSize / num_classes)\n",
    "    numEachClass_unlabeled_labeled = [numEachClass - int(numEachClass*labeled_rate), int(numEachClass*labeled_rate)]\n",
    "#     print(numEachClass)\n",
    "#     print(numEachClass_unlabeled_labeled)\n",
    "    returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "    returnLabel = np.zeros((0, num_classes))\n",
    "    returnLabeledMask = np.zeros((0))\n",
    "    for i in range(num_classes):\n",
    "        for j in [0,1]: #for unlabeled and labeled\n",
    "            indices = []\n",
    "            for k in range(len(train_mask_by_class[i])):\n",
    "                if train_mask_by_class[i][k] == j:\n",
    "                    indices.append(k)\n",
    "#             print('label status_'+str(j))\n",
    "#             print(indices)\n",
    "            np.random.shuffle(indices)\n",
    "            selectIndices = indices[0:numEachClass_unlabeled_labeled[j]]\n",
    "#             print('selected')\n",
    "#             print(selectIndices)\n",
    "            train_data = train_data_by_class[i]\n",
    "            train_label = train_label_by_class[i]\n",
    "            train_mask = train_mask_by_class[i]\n",
    "            \n",
    "            returnData = np.concatenate((returnData, train_data[selectIndices]))\n",
    "            returnLabel = np.concatenate((returnLabel, train_label[selectIndices]))\n",
    "            returnLabeledMask = np.concatenate((returnLabeledMask, train_mask[selectIndices]))\n",
    "    return returnData, returnLabel, returnLabeledMask\n",
    "\n",
    "# Input: c_ul is the relative portion ratio related to each class in a batch\n",
    "def get_balance_train_batch_3(train_data_by_class, train_label_by_class, train_mask_by_class, batchSize, c_ul):\n",
    "    numEachPortion = int(batchSize/(num_classes+1+c_ul)) # One portion of fake,c_ul portion of unlabeled\n",
    "    returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "    returnLabel = np.zeros((0, num_classes))\n",
    "    returnLabeledMask = np.zeros((0))\n",
    "    # First load labeled Data (mask is 1)\n",
    "    for i in range(num_classes):\n",
    "        labeled_is = np.random.choice(labeled_indices_by_class[i], numEachPortion)\n",
    "        returnData = np.concatenate((returnData, train_data_by_class[i][labeled_is]))\n",
    "        returnLabel = np.concatenate((returnLabel, train_label_by_class[i][labeled_is]))\n",
    "        returnLabeledMask = np.concatenate((returnLabeledMask, np.ones(numEachPortion))) # 1 for labeled\n",
    "    # Then load unlabeled Data (mask is 0)\n",
    "    if len(unlabeled_data) is not 0:\n",
    "        num_unlabeled = numEachPortion * c_ul\n",
    "        unlabeled_is = np.random.choice([i for i in range(len(unlabeled_data))], num_unlabeled)\n",
    "        returnData = np.concatenate((returnData, unlabeled_data[unlabeled_is]))\n",
    "        returnLabel = np.concatenate((returnLabel, np.zeros((num_unlabeled, num_classes))))\n",
    "        returnLabeledMask = np.concatenate((returnLabeledMask, np.zeros(num_unlabeled))) # 0 for unlabeled\n",
    "    return returnData, returnLabel, returnLabeledMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(test_data_by_class, test_label_by_class, batchSize):\n",
    "    XTest = np.zeros((0, x_width, x_height, num_channels))\n",
    "    yTest = np.zeros((0, num_classes))\n",
    "    for i in range(num_classes):\n",
    "        XTest = np.concatenate((XTest, test_data_by_class[i]))\n",
    "        yTest = np.concatenate((yTest, test_label_by_class[i]))\n",
    "    #first shuffle the indices:\n",
    "    XTestRandom, yTestRandom = shuffle_data(XTest, yTest, None);\n",
    "    #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "    counter = 0;\n",
    "    while True:\n",
    "        if counter >= len(yTest):\n",
    "            break;\n",
    "        returnXTest = XTestRandom[counter:counter + batchSize];\n",
    "        returnYTest = yTestRandom[counter:counter + batchSize];\n",
    "        counter = counter + batchSize;\n",
    "        yield returnXTest, returnYTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label, mask = get_balance_train_batch_3(train_data_by_class, train_label_by_class, train_mask_by_class, 60, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, label, mask = get_balance_train_batch_2(train_data_by_class, train_label_by_class, train_mask_by_class, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def D(x, dropout_rate, is_training, reuse = True, print_summary = True):\n",
    "    # discriminator (x -> n + 1 class)\n",
    "\n",
    "    with tf.variable_scope('Discriminator', reuse = reuse) as scope:\n",
    "        # layer1 - do not use Batch Normalization on the first layer of Discriminator\n",
    "        conv1 = tf.layers.conv2d(x, 32, [3, 3],\n",
    "                                 strides = [2, 2],\n",
    "                                 padding = 'same')\n",
    "        lrelu1 = tf.maximum(0.2 * conv1, conv1) #leaky relu\n",
    "        dropout1 = tf.layers.dropout(lrelu1, dropout_rate)\n",
    "\n",
    "        # layer2\n",
    "        conv2 = tf.layers.conv2d(dropout1, 64, [3, 3],\n",
    "                                 strides = [2, 2],\n",
    "                                 padding = 'same')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training, momentum=0.8)\n",
    "        lrelu2 = tf.maximum(0.2 * batch_norm2, batch_norm2)\n",
    "        dropout2 = tf.layers.dropout(lrelu2, dropout_rate)\n",
    "\n",
    "        # layer3\n",
    "        conv3 = tf.layers.conv2d(lrelu2, 64, [3, 3],\n",
    "                                 strides = [1, 1],\n",
    "                                 padding = 'same')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
    "        lrelu3 = tf.maximum(0.2 * batch_norm3, batch_norm3)\n",
    "        dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
    "        #dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
    "        #dropout3 = tf.layers.dropout(lrelu2, dropout_rate)\n",
    "        # layer 4\n",
    "        #conv4 = tf.layers.conv2d(dropout3, 256, [3, 3],\n",
    "                                #strides = [1, 1],\n",
    "                                #padding = 'same')\n",
    "        # do not use batch_normalization on this layer - next layer, \"flatten5\",\n",
    "        # will be used for \"Feature Matching\"\n",
    "        #lrelu4 = tf.maximum(0.2 * conv4, conv4)\n",
    "\n",
    "        # layer 5\n",
    "        flatten_length = dropout3.get_shape().as_list()[1] * \\\n",
    "                         dropout3.get_shape().as_list()[2] * dropout3.get_shape().as_list()[3]\n",
    "        flatten5 = tf.reshape(dropout3, (-1, flatten_length)) # used for \"Feature Matching\" \n",
    "        fc5 = tf.layers.dense(flatten5, (num_classes + 1))\n",
    "        output = tf.nn.softmax(fc5, name=\"D_output\")\n",
    "        \n",
    "        assert output.get_shape()[1:] == [num_classes + 1]\n",
    "\n",
    "        if print_summary:\n",
    "            print('Discriminator summary:\\n x: %s\\n' \\\n",
    "                  ' D1: %s\\n D2: %s\\n D3: %s\\n D4: %s\\n' %(x.get_shape(), \n",
    "                                                           dropout1.get_shape(),\n",
    "                                                           lrelu2.get_shape(), \n",
    "                                                           dropout3.get_shape(),\n",
    "                                                           lrelu4.get_shape()))\n",
    "        #return flatten5, fc5, output\n",
    "        #debug: return each layer's output --BZ Nov 28, 2019\n",
    "        return flatten5, fc5, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def G(z, is_training, reuse = False, print_summary = False):\n",
    "    # generator (z -> x)\n",
    "\n",
    "    with tf.variable_scope('Generator', reuse = reuse) as scope:\n",
    "        #z is 100*1\n",
    "        fc1 = tf.layers.dense(z, 32*32*128)\n",
    "        # layer 0\n",
    "        z_reshaped = tf.reshape(fc1, [-1, 32, 32, 128])\n",
    "\n",
    "    \n",
    "        \n",
    "        # layer 1\n",
    "        deconv1 = tf.layers.conv2d_transpose(z_reshaped,\n",
    "                                             filters = 128,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [2, 2],\n",
    "                                             padding = 'same')\n",
    "        batch_norm1 = tf.layers.batch_normalization(deconv1, training = is_training, momentum=0.8)\n",
    "        relu1 = tf.nn.relu(batch_norm1)\n",
    "        #64*64*64\n",
    "        # layer 2\n",
    "        deconv2 = tf.layers.conv2d_transpose(relu1,\n",
    "                                             filters = 64,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [2, 2],\n",
    "                                             padding = 'same')\n",
    "        batch_norm2 = tf.layers.batch_normalization(deconv2, training = is_training, momentum=0.8)\n",
    "        relu2 = tf.nn.relu(batch_norm2)\n",
    "        #128*128*3\n",
    "        # layer 3\n",
    "        #deconv3 = tf.layers.conv2d_transpose(relu2,\n",
    "                                             #filters = 64,\n",
    "                                             #kernel_size = [3, 3],\n",
    "                                             #strides = [1, 1],\n",
    "                                             #padding = 'same')\n",
    "        #batch_norm3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
    "        #relu3 = tf.nn.relu(batch_norm3)\n",
    "\n",
    "        # layer 4 - do not use Batch Normalization on the last layer of Generator\n",
    "        deconv4 = tf.layers.conv2d_transpose(relu2,\n",
    "                                             filters = num_channels,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [1, 1],\n",
    "                                             padding = 'same')\n",
    "        tanh4 = tf.tanh(deconv4, name=\"G_output\")\n",
    "\n",
    "        assert tanh4.get_shape()[1:] == [x_height, x_width, num_channels]\n",
    "        if print_summary:\n",
    "            print('Generator summary:\\n z: %s\\n' \\\n",
    "                  ' G0: %s\\n G1: %s\\n G2: %s\\n G3: %s\\n G4: %s\\n' %(z.get_shape(),\n",
    "                                                                    z_reshaped.get_shape(),\n",
    "                                                                    relu1.get_shape(),\n",
    "                                                                    relu2.get_shape(),\n",
    "                                                                    relu3.get_shape(),\n",
    "                                                                    tanh4.get_shape()))\n",
    "        return tanh4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build model for each batch using D() and G() functions\n",
    "def build_model(x_real, z, label, dropout_rate, is_training, print_summary = False):\n",
    "    # build model\n",
    "    #Discriminator on real data (labeled and unlabeled)  flatten5, fc5, output\n",
    "    D_real_features, D_real_logit, D_real_prob = D(x_real, dropout_rate, is_training,\n",
    "                                                   reuse = False, print_summary = print_summary)\n",
    "    #generate fake images\n",
    "    x_fake = G(z, is_training, reuse = False, print_summary = print_summary)\n",
    "    #Discriminator for fake images\n",
    "    D_fake_features, D_fake_logit, D_fake_prob = D(x_fake, dropout_rate, is_training,\n",
    "                                                   reuse = True, print_summary = print_summary)\n",
    "\n",
    "    return D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, x_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model that only contains the discriminator for production model\n",
    "def build_model_production(x_real, label, dropout_rate, is_training, print_summary = False):\n",
    "    x, dropout1, batch_norm2, dropout3, dropout4, D_real_features, D_real_logit, D_real_prob, conv1 = D(x_real, dropout_rate, is_training,\n",
    "                                                   reuse = False, print_summary = print_summary)\n",
    "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),#arg max returns the indices--Bill Zhai Aug, 2019\n",
    "                                  tf.argmax(label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "    return x, dropout1, batch_norm2, dropout3, dropout4, D_real_features, D_real_logit, D_real_prob, accuracy, correct_prediction, conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_labels(label):\n",
    "    # add extra label for telling apart fake data from real data\n",
    "    #essentially appending another column to the very end of the matrix which is for the 'fake' class\n",
    "    #uses one hot, so this newly appended column is all zeros--Bill Zhai Aug, 2019\n",
    "    extended_label = tf.concat([label, tf.zeros([tf.shape(label)[0], 1])], axis = 1)\n",
    "\n",
    "    return extended_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
    "                  D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
    "    epsilon = 1e-8 # used to avoid NAN loss\n",
    "    # *** Discriminator loss ***\n",
    "    # supervised loss\n",
    "    # which class the real data belongs to\n",
    "    tmp = tf.nn.softmax_cross_entropy_with_logits(logits = D_real_logit,#cross_entrypy_with_logits is the only function available\n",
    "                                                  labels = extended_label)\n",
    "    #question: is this an over simplification?--no, becuase tmp is a vector: num_samples_in_batch * 1\n",
    "    D_L_supervised = tf.reduce_sum(labeled_mask * tmp) / (tf.reduce_sum(labeled_mask)+1e-6) # to ignore unlabeled data\n",
    "                                                                                     \n",
    "\n",
    "    # unsupervised loss\n",
    "    # data is real\n",
    "    prob_real_be_real = 1 - D_real_prob[:, -1] + epsilon #\"-1\" signifies the last column which is probabilities of being \"fake\"\n",
    "    tmp_log = tf.log(prob_real_be_real)\n",
    "    D_L_unsupervised1 = -1 * tf.reduce_mean(tmp_log)\n",
    "\n",
    "    # data is fake\n",
    "    prob_fake_be_fake = D_fake_prob[:, -1] + epsilon\n",
    "    tmp_log = tf.log(prob_fake_be_fake)\n",
    "    D_L_unsupervised2 = -1 * tf.reduce_mean(tmp_log)\n",
    "\n",
    "    D_L = D_L_supervised + D_L_unsupervised1 + D_L_unsupervised2\n",
    "\n",
    "    # *** Generator loss ***\n",
    "    # fake data is mistaken to be real\n",
    "    prob_fake_be_real = 1 - D_fake_prob[:, -1] + epsilon\n",
    "    tmp_log =  tf.log(prob_fake_be_real)\n",
    "    G_L1 = -1 * tf.reduce_mean(tmp_log)\n",
    "\n",
    "    # Feature Maching\n",
    "    tmp1 = tf.reduce_mean(D_real_features, axis = 0)\n",
    "    tmp2 = tf.reduce_mean(D_fake_features, axis = 0)\n",
    "    G_L2 = tf.reduce_mean(tf.square(tmp1 - tmp2))\n",
    "\n",
    "    G_L = G_L1 + G_L2\n",
    "\n",
    "    # accuracy--This is cross validation accuracy within the training set--Nov, 2019\n",
    "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),#arg max returns the indices--Bill Zhai Aug, 2019\n",
    "                                  tf.argmax(extended_label[:, :-1], 1), name='correct_prediction')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #cast boolean to float32--Bill Zhai Aug, 2019\n",
    "    \n",
    "    \n",
    "    return D_L_supervised, D_L_unsupervised1, D_L_unsupervised2, D_L, G_L, accuracy, correct_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimizer(D_Loss, G_Loss, D_learning_rate, G_learning_rate):\n",
    "    # D and G optimizer\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        all_vars = tf.trainable_variables()\n",
    "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
    "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
    "        #print('D_vars:')\n",
    "        #print(D_vars)\n",
    "        D_optimizer = tf.train.AdamOptimizer(D_learning_rate).minimize(D_Loss, var_list = D_vars)\n",
    "        G_optimizer = tf.train.AdamOptimizer(G_learning_rate).minimize(G_Loss, var_list = G_vars)\n",
    "        return D_optimizer, G_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Assume len(data) > 5 --BZ Nov 30, 2019\n",
    "\n",
    "def plot_fake_data(data, epoch):\n",
    "    # visualize some data generated by G\n",
    "    data = (1/2.5) * data + 0.5\n",
    "    fig, axs = plt.subplots(len(data), figsize=(30,30))\n",
    "    cnt = 0\n",
    "    for j in range(len(data)):\n",
    "        #print(j)\n",
    "        #print(data[cnt, :, :, :])\n",
    "        axs[j].imshow(data[cnt, :, :, :])\n",
    "        axs[j].axis(\"off\")\n",
    "        cnt = cnt + 1\n",
    "    print('graphed!')        \n",
    "    if not os.path.exists(\"./training_fake_figure\"):\n",
    "        os.mkdir(\"./training_fake_figure\");\n",
    "    plt.savefig(\"training_fake_figure/%d.jpg\" % epoch)\n",
    "    plt.close()\n",
    "\n",
    "def save_fake_image(data, epoch):\n",
    "    #if not os.path.exists(\"./training_fake_imageMatrix\"):\n",
    "        #os.mkdir(\"./training_fake_imageMatrix\");\n",
    "    #np.save(\"./training_fake_imageMatrix/epoch_\"+str(epoch), data)\n",
    "    #only plot the last image of the batch\n",
    "    if not os.path.exists(\"./training_fake_figure\"):\n",
    "        os.mkdir(\"./training_fake_figure\");\n",
    "    plt.imshow(data[-1]/2+0.5)\n",
    "    plt.set_cmap('hot')\n",
    "    plt.axis('off')\n",
    "    #print(data[-1])\n",
    "    plt.savefig(\"training_fake_figure/%d.jpg\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_model_on_improvement(file_path, sess, cv_acc, cv_accs):\n",
    "  #  # save model when there is improvemnet in cv_acc value\n",
    "    if cv_accs == [] or cv_acc >= np.max(cv_accs):\n",
    "        saver = tf.train.Saver(max_to_keep = 1)\n",
    "        saver.save(sess, file_path)\n",
    "        print('Model saved')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the top 5 best model based on validation accuracy\n",
    "def save_model_top_five(folder_path, sess, cv_acc, cv_accs):\n",
    "    #cv_acc is inside cv_accs\n",
    "    if not os.path.exists(folder_path+'/'+task_name):\n",
    "        os.mkdir(folder_path+'/'+task_name)\n",
    "    sortedAccs = np.sort(cv_accs)\n",
    "    for i in range(len(cv_accs)):\n",
    "        if i >= 5:\n",
    "            return\n",
    "        if cv_acc >= sortedAccs[i]:\n",
    "            saver = tf.train.Saver(max_to_keep = 1)\n",
    "            saver.save(sess, folder_path+'/'+task_name+'/'+'_top_'+str(i+1)+'_SSL_GAN.ckpt')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model based on TPR and TNR criteria\n",
    "def save_model_TPR_TNR(folder_path, sess, epoch, cv_acc, cv_accs, TPR, TNR, TPRs):\n",
    "    if not os.path.exists(folder_path+'/'+task_name):\n",
    "        os.mkdir(folder_path+'/'+task_name)\n",
    "    sortedTPRs = np.sort(TPRs)[::-1] # Sort in Descending Order!\n",
    "    for i in range(len(sortedTPRs)):\n",
    "        if i >= 5:\n",
    "            return\n",
    "        #if TPR >= sortedTPRs[i] and TPR >= 0.88 and TNR >= 0.91:\n",
    "        if TPR >= sortedTPRs[i]:\n",
    "            print('save model')\n",
    "            saver = tf.train.Saver(max_to_keep = 1)\n",
    "            saver.save(sess, folder_path+'/'+task_name+'/'+'_TPR_top_'+str(i+1)+'_epoch_'+str(epoch)+'_SSL_GAN.ckpt')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file_path, sess):\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    saver.save(sess, file_path)\n",
    "    print('Every 500 model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_loss_acc_binary(file_path, epoch, train_loss_D, train_loss_G, train_Acc,\n",
    "                 cv_Acc, CM_0_0, CM_0_1, CM_1_0, CM_1_1, TPR, TNR, log_mode = 'a'):\n",
    "    # log train and cv losses as well as accuracy\n",
    "    mode = log_mode if epoch == 0 else 'a'\n",
    "\n",
    "    with open(file_path, mode) as f:\n",
    "        if mode == 'w':\n",
    "            header = 'epoch, train_loss_D, train_loss_G,' \\\n",
    "                     'train_Acc, val_Acc, CM_0_0, CM_0_1, CM_1_0, CM_1_1, TPR, TNR\\n'\n",
    "            f.write(header)\n",
    "\n",
    "        line = '%d, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f\\n' %(epoch, train_loss_D, train_loss_G, train_Acc,\n",
    "                                                cv_Acc, CM_0_0, CM_0_1, CM_1_0, CM_1_1, TPR, TNR)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Modified by BZ on Nov 3, 2019\n",
    "#Note: predictions vector has binary entries: 1 corresponds to correct prediction, 0 is wrong prediction\n",
    "def compute_val_accuracy(correct_predictions):\n",
    "    return np.sum(correct_predictions)/len(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training function--Bill Zhai Aug 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SSL_GAN(batch_size, num_epochs, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class):\n",
    "    # train Semi-Supervised Learning GAN\n",
    "    train_D_losses, train_G_losses, train_Accs = [], [], []\n",
    "    val_D_losses, val_G_losses, val_Accs, TPRs = [], [], [], []\n",
    "    \n",
    "    cv_size = batch_size\n",
    "    num_train_exs = numTrain\n",
    "    num_val_exs = numTest\n",
    "    print(batch_size)\n",
    "    print(\"num_train_exs: \", num_train_exs)\n",
    "    print(\"num_val_exs: \", num_val_exs)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, name = 'x', shape = [None, x_height, x_width, num_channels])\n",
    "    label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes]) # one hot label--BZ, August, 2019\n",
    "    labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
    "    z = tf.placeholder(tf.float32, name = 'z', shape = [None, latent_size])#one 1-d noise vector per training example\n",
    "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    G_learning_rate = tf.placeholder(tf.float32, name = 'G_learning_rate')\n",
    "    D_learning_rate = tf.placeholder(tf.float32, name = 'D_learning_rate')\n",
    "\n",
    "    model = build_model(x, z, label, dropout_rate, is_training, print_summary = False)\n",
    "    D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, fake_data = model\n",
    "    extended_label = prepare_labels(label) #is only for real image data\n",
    "    loss_acc  = loss_accuracy(D_real_features, D_real_logit, D_real_prob,\n",
    "                              D_fake_features, D_fake_logit, D_fake_prob,\n",
    "                              extended_label, labeled_mask)\n",
    "    _, _, _, D_L, G_L, accuracy, correct_prediction = loss_acc\n",
    "    D_optimizer, G_optimizer = optimizer(D_L, G_L, G_learning_rate, D_learning_rate)\n",
    "\n",
    "    \n",
    "#     validation_generator = get_batch(data_path, label_path, num_val_exs, num_train_exs, True)\n",
    "\n",
    "    print('training....')\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:       \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #mnist_set = get_data()\n",
    "\n",
    "        t_total = 0\n",
    "        #changed to iterating on number of epochs!\n",
    "        iter_count = 0\n",
    "        iter_since_last_val = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            for iteration in range(int(numTrain/(batch_size*(num_classes+c_ul)/(num_classes+1+c_ul)))):\n",
    "            #batch_num = 0 #added--BZ Nov 2, 2019\n",
    "            #training_generator = get_training_batch_and_labeled_mask(X_train, y_train, LABELED_MASK, batch_size);\n",
    "                train_batch_x, train_batch_y, train_batch_mask = get_balance_train_batch_3(train_data_by_class, train_label_by_class, train_mask_by_class, batch_size, c_ul)            #for train_batch_x, train_batch_y, train_batch_mask in training_generator:\n",
    "                t_start = time.time()\n",
    "                #batch_z = np.random.uniform(-1.0, 1.0, size = (batch_size, latent_size)) #\n",
    "                #batch_z = np.random.normal(0, 1, size = (int(batch_size/num_classes), latent_size))\n",
    "                batch_z = np.random.normal(0, 1, size = (int(batch_size/(num_classes+1+c_ul)), latent_size))\n",
    "                #function is to be modified as labeled mask should stay with \"labeled\" images which are shuffled BZ--August, 2019\n",
    "                #mask = get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize);--marked as solved BZ, Nov, 2019\n",
    "                train_feed_dictionary = {x:  normalize(train_batch_x),\n",
    "                                         z: batch_z,\n",
    "                                         label: train_batch_y,\n",
    "                                         labeled_mask: train_batch_mask,\n",
    "                                         dropout_rate: 0.25,\n",
    "                                         G_learning_rate: 2e-5,\n",
    "                                         D_learning_rate: 2e-5,\n",
    "                                         is_training: True}\n",
    "\n",
    "                D_optimizer.run(feed_dict = train_feed_dictionary)\n",
    "                G_optimizer.run(feed_dict = train_feed_dictionary)\n",
    "\n",
    "                train_D_loss = D_L.eval(feed_dict = train_feed_dictionary)\n",
    "                train_G_loss = G_L.eval(feed_dict = train_feed_dictionary)\n",
    "                train_accuracy = accuracy.eval(feed_dict = train_feed_dictionary)\n",
    "                t_total += (time.time() - t_start)\n",
    "\n",
    "                #debug:\n",
    "                #trainDLogits = D_real_logit.eval(feed_dict = train_feed_dictionary)\n",
    "                #trainRealFeatures = D_real_features.eval(feed_dict = train_feed_dictionary)\n",
    "            \n",
    "                \n",
    "                \n",
    "                #print('Epoch: '+str(epoch)+\" Iter: \"+str(iteration)+\" Time: \"+str(t_total)+\" train_G_loss: \"\n",
    "                    #  +str(train_G_loss)+\" train_D_loss: \"+str(train_D_loss)+\" train_accuracy: \"+str(train_accuracy))\n",
    "\n",
    "                #print('trainDLogits:')\n",
    "                #print(trainDLogits)\n",
    "                #print('trainFeatures:')\n",
    "                #print(trainRealFeatures)\n",
    "                #print('xinput')\n",
    "                #print(xinput.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout1')\n",
    "                #print(dropout1.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('batch_norm2')\n",
    "                #print(batch_norm2.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout3')\n",
    "                #print(dropout3.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout4')\n",
    "                #print(dropout4.eval(feed_dict = train_feed_dictionary))\n",
    "\n",
    "                train_D_losses.append(train_D_loss)\n",
    "                train_G_losses.append(train_G_loss)\n",
    "                train_Accs.append(train_accuracy)\n",
    "                #batch_num = batch_num + 1;\n",
    "            # Validation at the end of each epoch--BZ, Nov, 2019\n",
    "\n",
    "            \n",
    "            test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "            val_correct_preds = []\n",
    "            predictions = []\n",
    "            labels = np.zeros((0, num_classes))\n",
    "            for test_batch_x, test_batch_y in test_generator:\n",
    "            #test_batch_generator = get_test_batch(X_test, y_test, batch_size);#added--BZ, Nov 2, 2019\n",
    "            #for test_batch_x, test_batch_y in test_batch_generator:\n",
    "                val_batch_z = np.random.normal(0, 1, size = (len(test_batch_y), latent_size))\n",
    "                mask = np.ones(len(test_batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "                val_feed_dictionary = {x: normalize(test_batch_x),\n",
    "                                       z: val_batch_z,\n",
    "                                       label: test_batch_y,\n",
    "                                       labeled_mask: mask,\n",
    "                                       dropout_rate: 0.0,\n",
    "                                       is_training: False}\n",
    "\n",
    "\n",
    "                val_D_loss = D_L.eval(feed_dict = val_feed_dictionary)\n",
    "                val_G_loss = G_L.eval(feed_dict = val_feed_dictionary)\n",
    "\n",
    "                val_correct_pred = correct_prediction.eval(feed_dict = val_feed_dictionary)\n",
    "                val_correct_preds = np.concatenate((val_correct_preds, val_correct_pred))\n",
    "                    \n",
    "                val_D_real_prob = D_real_prob.eval(feed_dict = val_feed_dictionary)\n",
    "                predictions = np.concatenate((predictions, np.argmax(val_D_real_prob[:, :-1], axis = 1)))\n",
    "                labels = np.concatenate((labels, test_batch_y))\n",
    "            \n",
    "            val_accuracy = compute_val_accuracy(val_correct_preds)\n",
    "            val_Accs.append(val_accuracy)\n",
    "    \n",
    "            CM = confusion_matrix(np.argmax(labels, axis = 1), predictions)\n",
    "            TPR = CM[1][1]/(CM[1][0]+CM[1][1])\n",
    "            TNR = CM[0][0]/(CM[0][0]+CM[0][1])\n",
    "            TPRs.append(TPR)\n",
    "            #print(val_correct_preds);\n",
    "            #print('validation_acc: %f' %(val_accuracy))\n",
    "            log_loss_acc_binary(log_path, epoch, train_D_loss, train_G_loss, train_accuracy,\n",
    "                 val_accuracy, CM[0][0], CM[0][1], CM[1][0], CM[1][1], TPR, TNR, 'w')\n",
    "    \n",
    "            save_model_TPR_TNR(model_path, sess, epoch, val_accuracy, val_Accs, TPR, TNR, TPRs)\n",
    "            \n",
    "            fakes = fake_data.eval(feed_dict = val_feed_dictionary)\n",
    "            save_fake_image(fakes, epoch)\n",
    "            #confusion matrix\n",
    "            #print('epoch'+str(epoch)+' CM:')\n",
    "            #print(confusion_matrix(np.argmax(labels, axis = 1), predictions, normalize = 'true'))\n",
    "                \n",
    "        \n",
    "    return train_D_losses, train_G_losses, train_Accs, val_Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "num_train_exs:  10200\n",
      "num_val_exs:  5100\n",
      "WARNING:tensorflow:From <ipython-input-12-c2ae7e700ea5>:8: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/yuqing/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-12-c2ae7e700ea5>:10: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-12-c2ae7e700ea5>:16: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-12-c2ae7e700ea5>:40: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-b060878c3880>:17: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d_transpose instead.\n",
      "WARNING:tensorflow:From <ipython-input-17-4e9e08249fff>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/yuqing/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "training....\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n",
      "save model\n"
     ]
    }
   ],
   "source": [
    "train_SSL_GAN(60, 340, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions, labels, numClasses):\n",
    "            #pred 0   #pred 1\n",
    "    #real 0\n",
    "    #real 1\n",
    "    matrix = np.zeros((numClasses, numClasses))\n",
    "    for n in range(len(predictions)):\n",
    "        matrix[int(labels[n]), int(predictions[n])] += 1\n",
    "    return matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Retrieve best DCGAN model and run the whole training set or test set to get the feature map vectors, \n",
    "#which are later used to either\n",
    "#i. train a Random Forest Model\n",
    "#or\n",
    "#ii. run on test set\n",
    "#start new session:\n",
    "def testModel(test_data_by_class, test_label_by_class, batch_size):\n",
    "    labels = np.zeros((0,num_classes))\n",
    "    predictions = np.zeros((0))\n",
    "    correct_preds = np.zeros((0))\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess: \n",
    "        #bestModel = tf.train.import_meta_graph('./savedModels/GAN/'+str(datetime.date.today())+'_SSL_GAN.ckpt.meta')#load graph\n",
    "        bestModel = tf.train.import_meta_graph('./savedModels/GAN/2020-01-27_SSL_GAN.ckpt.meta')#load graph\n",
    "\n",
    "        bestModel.restore(sess,tf.train.latest_checkpoint('./savedModels/GAN'))#load parameters\n",
    "        #feed placeholders:\n",
    "        x = sess.graph.get_tensor_by_name(\"x:0\")\n",
    "        label = sess.graph.get_tensor_by_name(\"label:0\")\n",
    "        labeled_mask = sess.graph.get_tensor_by_name(\"labeled_mask:0\")\n",
    "        dropout_rate = sess.graph.get_tensor_by_name(\"dropout_rate:0\")\n",
    "        is_training = sess.graph.get_tensor_by_name(\"is_training:0\")\n",
    "        #output metrics of interest:\n",
    "        D_real_prob = sess.graph.get_tensor_by_name(\"Discriminator/D_output:0\")\n",
    "        G_output = sess.graph.get_tensor_by_name(\"Generator/G_output:0\")\n",
    "        correct_prediction = sess.graph.get_tensor_by_name(\"correct_prediction:0\")\n",
    "#         prediction = tf.argmax(D_real_prob, axis = 1)\n",
    "        prediction = tf.argmax( D_real_prob[:, :-1], axis = 1)\n",
    "\n",
    "        test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "        \n",
    "        #for debugging: output weight names\n",
    "        weights = [v.eval(session=sess) for v in tf.trainable_variables()]\n",
    "        weightNames = [v for v in tf.trainable_variables()]\n",
    "        i = 0\n",
    "        for batch_x, batch_y in test_generator:\n",
    "            #print('batch_y:')\n",
    "            #print(batch_y)\n",
    "            mask = np.ones(len(batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "            feed_dictionary = {x: normalize(batch_x),\n",
    "                               label: batch_y,\n",
    "                               labeled_mask: mask,\n",
    "                               dropout_rate: 0.0,\n",
    "                               is_training: False}\n",
    "            \n",
    "            #accuracy_eval = accuracy.eval(feed_dict = feed_dictionary)\n",
    "            correct_pred_eval = correct_prediction.eval(feed_dict = feed_dictionary)\n",
    "            D_real_prob_eval = D_real_prob.eval(feed_dict = feed_dictionary)\n",
    "            \n",
    "            correct_preds = np.concatenate((correct_preds, correct_pred_eval))\n",
    "            prediction_eval = prediction.eval(feed_dict=feed_dictionary)\n",
    "            labels = np.concatenate((labels, batch_y))#array of k+1 one hot\n",
    "            predictions = np.concatenate((predictions, prediction_eval))#array of ints\n",
    "            #print('real features: batch'+str(i))\n",
    "            #print(D_real_features_eval)\n",
    "            #print('accuracy: '+str(accuracy_eval))\n",
    "            #print(str(D_real_features_eval.shape))\n",
    "            #print('labels'+str(labels.shape))\n",
    "            #print('featuremaps'+str(feature_maps.shape))\n",
    "            #print('real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            #print('dropout1:')\n",
    "            #print(dropout1.eval(feed_dict = feed_dictionary))\n",
    "            #print('conv1:')\n",
    "            #print(conv1.eval(feed_dict = feed_dictionary))\n",
    "            #print('x:')\n",
    "            #print(x.eval(feed_dict = feed_dictionary))\n",
    "            #print('D_real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            i = i + 1\n",
    "        sess.close()\n",
    "    total_accuracy = np.sum(correct_preds)/len(correct_preds)\n",
    "    print('total accuracy: '+str(total_accuracy))\n",
    "#     print('labels:')\n",
    "#     print(labels)\n",
    "    print('predictions:')\n",
    "    print(predictions)\n",
    "    print('Confusion Matirx:')\n",
    "#     print(confusion_matrix(predictions, np.argmax(labels, axis = 1), num_classes+1))\n",
    "    print(confusion_matrix(np.argmax(labels, axis = 1), predictions))\n",
    "#     return labels, weights, weightNames\n",
    "    return predictions, labels, weights, weightNames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test model:\n",
    "predictions, labels_test, params, paramNames = testModel(test_data_by_class, test_label_by_class, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "def loss_accuracy_baseline(D_real_features, D_real_logit, D_real_prob, extended_label, labeled_mask):\n",
    "    epsilon = 1e-8 # used to avoid NAN loss\n",
    "    # *** Discriminator loss ***\n",
    "    # supervised loss\n",
    "    # which class the real data belongs to\n",
    "    tmp = tf.nn.softmax_cross_entropy_with_logits(logits = D_real_logit,#cross_entrypy_with_logits is the only function available\n",
    "                                                  labels = extended_label)\n",
    "    #question: is this an over simplification?--no, becuase tmp is a vector: num_samples_in_batch * 1\n",
    "    D_L_supervised = tf.reduce_sum(labeled_mask * tmp) / (tf.reduce_sum(labeled_mask)+1e-6) # to ignore unlabeled data\n",
    "                                                                                     \n",
    "    # accuracy--This is cross validation accuracy within the training set--Nov, 2019\n",
    "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),#arg max returns the indices--Bill Zhai Aug, 2019\n",
    "                                  tf.argmax(extended_label[:, :-1], 1), name='correct_prediction')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #cast boolean to float32--Bill Zhai Aug, 2019\n",
    "    \n",
    "    \n",
    "    return D_L_supervised, accuracy, correct_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_baseline(D_Loss, D_learning_rate):\n",
    "    # D and G optimizer\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        all_vars = tf.trainable_variables()\n",
    "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
    "        D_optimizer = tf.train.AdamOptimizer(D_learning_rate).minimize(D_Loss, var_list = D_vars)\n",
    "        return D_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model for each batch using D() and G() functions\n",
    "def build_model_baseline(x_real, label, dropout_rate, is_training, print_summary = False):\n",
    "    # build model\n",
    "    #Discriminator on real data (labeled and unlabeled)  flatten5, fc5, output\n",
    "    D_real_features, D_real_logit, D_real_prob = D(x_real, dropout_rate, is_training,\n",
    "                                                   reuse = False, print_summary = print_summary)\n",
    "    return D_real_features, D_real_logit, D_real_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(batch_size, num_epochs, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class):\n",
    "    # train Semi-Supervised Learning GAN\n",
    "    train_D_losses, train_Accs = [], []\n",
    "    val_D_losses, val_Accs = [], []\n",
    "\n",
    "    cv_size = batch_size\n",
    "    num_train_exs = numTrain\n",
    "    num_val_exs = numTest\n",
    "    print(batch_size)\n",
    "    print(\"num_train_exs: \", num_train_exs)\n",
    "    print(\"num_val_exs: \", num_val_exs)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, name = 'x', shape = [None, x_height, x_width, num_channels])\n",
    "    label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes]) # one hot label--BZ, August, 2019\n",
    "    labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
    "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    D_learning_rate = tf.placeholder(tf.float32, name = 'D_learning_rate')\n",
    "\n",
    "    model = build_model_baseline(x, label, dropout_rate, is_training, print_summary = False)\n",
    "    D_real_features, D_real_logit, D_real_prob = model\n",
    "    extended_label = prepare_labels(label) #is only for real image data\n",
    "    loss_acc  = loss_accuracy_baseline(D_real_features, D_real_logit, D_real_prob,\n",
    "                              extended_label, labeled_mask)\n",
    "    D_L, accuracy, correct_prediction = loss_acc\n",
    "    D_optimizer = optimizer_baseline(D_L, D_learning_rate)\n",
    "\n",
    "    \n",
    "#     validation_generator = get_batch(data_path, label_path, num_val_exs, num_train_exs, True)\n",
    "\n",
    "    print('training....')\n",
    "\n",
    "    with tf.Session() as sess:       \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #mnist_set = get_data()\n",
    "\n",
    "        t_total = 0\n",
    "        #changed to iterating on number of epochs!\n",
    "        iter_count = 0\n",
    "        iter_since_last_val = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            for iteration in range(int(numTrain/batch_size)):\n",
    "            #batch_num = 0 #added--BZ Nov 2, 2019\n",
    "            #training_generator = get_training_batch_and_labeled_mask(X_train, y_train, LABELED_MASK, batch_size);\n",
    "                train_batch_x, train_batch_y, train_batch_mask = get_balance_train_batch(train_data_by_class, train_label_by_class, train_mask_by_class, batch_size)\n",
    "            #for train_batch_x, train_batch_y, train_batch_mask in training_generator:\n",
    "                t_start = time.time()\n",
    "                #function is to be modified as labeled mask should stay with \"labeled\" images which are shuffled BZ--August, 2019\n",
    "                #mask = get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize);--marked as solved BZ, Nov, 2019\n",
    "                train_feed_dictionary = {x:  normalize(train_batch_x),\n",
    "                                         label: train_batch_y,\n",
    "                                         labeled_mask: train_batch_mask,\n",
    "                                         dropout_rate: 0.0,\n",
    "                                         D_learning_rate: 1e-5,\n",
    "                                         is_training: True}\n",
    "\n",
    "                D_optimizer.run(feed_dict = train_feed_dictionary)\n",
    "\n",
    "                train_D_loss = D_L.eval(feed_dict = train_feed_dictionary)\n",
    "                train_accuracy = accuracy.eval(feed_dict = train_feed_dictionary)\n",
    "                t_total += (time.time() - t_start)\n",
    "\n",
    "                print('Epoch: '+str(epoch)+\" Iter: \"+str(iteration)+\" Time: \"+str(t_total)+\" train_D_loss: \"+str(train_D_loss)+\" train_accuracy: \"+str(train_accuracy))\n",
    "\n",
    "                #print('trainDLogits:')\n",
    "                #print(trainDLogits)\n",
    "                #print('trainFeatures:')\n",
    "                #print(trainRealFeatures)\n",
    "                #print('xinput')\n",
    "                #print(xinput.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout1')\n",
    "                #print(dropout1.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('batch_norm2')\n",
    "                #print(batch_norm2.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout3')\n",
    "                #print(dropout3.eval(feed_dict = train_feed_dictionary))\n",
    "                #print('dropout4')\n",
    "                #print(dropout4.eval(feed_dict = train_feed_dictionary))\n",
    "\n",
    "                train_D_losses.append(train_D_loss)\n",
    "                train_Accs.append(train_accuracy)\n",
    "                #batch_num = batch_num + 1;\n",
    "            # Validation at the end of each epoch--BZ, Nov, 2019\n",
    "\n",
    "            \n",
    "            test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "            val_correct_preds = []\n",
    "            for test_batch_x, test_batch_y in test_generator:\n",
    "            #test_batch_generator = get_test_batch(X_test, y_test, batch_size);#added--BZ, Nov 2, 2019\n",
    "            #for test_batch_x, test_batch_y in test_batch_generator:\n",
    "                mask = np.ones(len(test_batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "                val_feed_dictionary = {x: normalize(test_batch_x),\n",
    "                                       label: test_batch_y,\n",
    "                                       labeled_mask: mask,\n",
    "                                       dropout_rate: 0.0,\n",
    "                                       is_training: False}\n",
    "\n",
    "\n",
    "                val_D_loss = D_L.eval(feed_dict = val_feed_dictionary)\n",
    "\n",
    "                val_correct_pred = correct_prediction.eval(feed_dict = val_feed_dictionary)\n",
    "                val_correct_preds = np.concatenate((val_correct_preds, val_correct_pred))\n",
    "                    \n",
    "            \n",
    "            val_accuracy = compute_val_accuracy(val_correct_preds)\n",
    "            val_Accs.append(val_accuracy)\n",
    "    \n",
    "            #print(val_correct_preds);\n",
    "            #print('validation_acc: %f' %(val_accuracy))\n",
    "            log_loss_acc('./baseline_log_'+str(datetime.date.today())+'.csv', epoch, train_D_loss, 0, train_accuracy,\n",
    "                 val_accuracy, 'w')\n",
    "    \n",
    "            save_model_on_improvement(baseline_path+'/'+str(datetime.date.today())+'_baseline.ckpt', sess, val_accuracy, val_Accs)\n",
    "            \n",
    "                \n",
    "        \n",
    "    return train_D_losses, train_Accs, val_Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baseline(64, 500000, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve best DCGAN model and run the whole training set or test set to get the feature map vectors, \n",
    "#which are later used to either\n",
    "#i. train a Random Forest Model\n",
    "#or\n",
    "#ii. run on test set\n",
    "#start new session:\n",
    "def testModel_baseline(test_data_by_class, test_label_by_class, batch_size):\n",
    "    labels = np.zeros((0,num_classes))\n",
    "    predictions = np.zeros((0))\n",
    "    correct_preds = np.zeros((0))\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess: \n",
    "        bestModel = tf.train.import_meta_graph('./savedModels/baseline/'+str(datetime.date.today())+'_baseline.ckpt.meta')#load graph\n",
    "        bestModel.restore(sess,tf.train.latest_checkpoint('./savedModels/baseline'))#load parameters\n",
    "        #feed placeholders:\n",
    "        x = sess.graph.get_tensor_by_name(\"x:0\")\n",
    "        label = sess.graph.get_tensor_by_name(\"label:0\")\n",
    "        labeled_mask = sess.graph.get_tensor_by_name(\"labeled_mask:0\")\n",
    "        dropout_rate = sess.graph.get_tensor_by_name(\"dropout_rate:0\")\n",
    "        is_training = sess.graph.get_tensor_by_name(\"is_training:0\")\n",
    "        #output metrics of interest:\n",
    "        D_real_prob = sess.graph.get_tensor_by_name(\"Discriminator/D_output:0\")\n",
    "        correct_prediction = sess.graph.get_tensor_by_name(\"correct_prediction:0\")\n",
    "#         prediction = tf.argmax(D_real_prob, axis = 1)\n",
    "        prediction = tf.argmax( D_real_prob[:, :-1], axis = 1)\n",
    "\n",
    "        test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "        \n",
    "        #for debugging: output weight names\n",
    "        weights = [v.eval(session=sess) for v in tf.trainable_variables()]\n",
    "        weightNames = [v for v in tf.trainable_variables()]\n",
    "        i = 0\n",
    "        for batch_x, batch_y in test_generator:\n",
    "            #print('batch_y:')\n",
    "            #print(batch_y)\n",
    "            mask = np.ones(len(batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "            feed_dictionary = {x: normalize(batch_x),\n",
    "                               label: batch_y,\n",
    "                               labeled_mask: mask,\n",
    "                               dropout_rate: 0.0,\n",
    "                               is_training: False}\n",
    "            \n",
    "            #accuracy_eval = accuracy.eval(feed_dict = feed_dictionary)\n",
    "            correct_pred_eval = correct_prediction.eval(feed_dict = feed_dictionary)\n",
    "            D_real_prob_eval = D_real_prob.eval(feed_dict = feed_dictionary)\n",
    "            \n",
    "            correct_preds = np.concatenate((correct_preds, correct_pred_eval))\n",
    "            prediction_eval = prediction.eval(feed_dict=feed_dictionary)\n",
    "            labels = np.concatenate((labels, batch_y))#array of k+1 one hot\n",
    "            predictions = np.concatenate((predictions, prediction_eval))#array of ints\n",
    "            #print('real features: batch'+str(i))\n",
    "            #print(D_real_features_eval)\n",
    "            #print('accuracy: '+str(accuracy_eval))\n",
    "            #print(str(D_real_features_eval.shape))\n",
    "            #print('labels'+str(labels.shape))\n",
    "            #print('featuremaps'+str(feature_maps.shape))\n",
    "            #print('real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            #print('dropout1:')\n",
    "            #print(dropout1.eval(feed_dict = feed_dictionary))\n",
    "            #print('conv1:')\n",
    "            #print(conv1.eval(feed_dict = feed_dictionary))\n",
    "            #print('x:')\n",
    "            #print(x.eval(feed_dict = feed_dictionary))\n",
    "            #print('D_real_prob:')\n",
    "            #print(D_real_prob_eval)\n",
    "            i = i + 1\n",
    "        sess.close()\n",
    "    total_accuracy = np.sum(correct_preds)/len(correct_preds)\n",
    "    print('total accuracy: '+str(total_accuracy))\n",
    "#     print('labels:')\n",
    "#     print(labels)\n",
    "    print('predictions:')\n",
    "    print(predictions)\n",
    "    print('Confusion Matirx:')\n",
    "#     print(confusion_matrix(predictions, np.argmax(labels, axis = 1), num_classes+1))\n",
    "    print(confusion_matrix(np.argmax(labels, axis = 1), predictions))\n",
    "#     return labels, weights, weightNames\n",
    "    return predictions, labels, weights, weightNames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel_baseline(test_data_by_class, test_label_by_class, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
